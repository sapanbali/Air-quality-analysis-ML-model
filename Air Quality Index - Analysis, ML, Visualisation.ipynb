{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a4c670fd76e030a071460082e533d4bbb6e5f7e"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from IPython.display import Image\n",
    "Image(url= \"https://images.mapsofindia.com/liveblog/2018/10/air-quality-index-of-the-biggest-cities-in-india-f1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa391a81acb97f71427f99e1d6bd24d6520c724f"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db3b931c2cf75e41051b15ebdcc95e78810360ac"
   },
   "source": [
    "# <p style=\"text-align: center;\"> Table of Contents </p>\n",
    "- ## 1. Introduction\n",
    "   - ### 1.1 [Abstract](#abstract)\n",
    "   - ### 1.2 [Importing Libraries](#importing_libraries)\n",
    "   - ### 1.3 [Dataset Summary](#dataset_summary)\n",
    "   - ### 1.4 [Dataset Cleaning](#dataset_cleaning)\n",
    "   - ### 1.5 [Exploratory Data Analysis (EDA)](#eda)\n",
    "        - ### 1.5.1[Pointplot](#pointplot) \n",
    "        - ### 1.5.2[Heatmap](#heatmap)\n",
    "        - ### 1.5.3[HeatMap Pivot](#heatmp)\n",
    "        - ### 1.5.4[Regplot](#Regplot)\n",
    "        - ### 1.5.5[Categorical Conversion](#catconversion)\n",
    "- ## 2. [Linear Regression](#LinearRegression)\n",
    "    - ### 2.1 [Linear Model 1](#LinearModel1)\n",
    "    - ### 2.2 [Linear Model 2](#LinearModel2)\n",
    "    - ### 2.3 [Linear Model 3](#LinearModel3)\n",
    "- ## 3. [Logistic Regression](#LogisticRegression)\n",
    "    - ### 3.1 [Logistic Model 1](#LogisticModel1)\n",
    "    - ### 3.2 [Logistic Model 2](#LogisticModel2)\n",
    "    - ### 3.3 [Logistic Model 3](#LogisticModel3) \n",
    "- ## 4. [Multicollinearity](#Multicollinearity)\n",
    "- ## 5. [Stepwise Regression](#StepwiseRegression)\n",
    "- ## 6. [Interaction Effect](#InteractionEffect)\n",
    "    - ### 6.1 [All Columns-NO2,RSPM, SPM](#AllColumns)\n",
    "    - ### 6.2 [RSPM & NO2](#RSPMNO2)\n",
    "    - ### 6.3 [SPM & NO2](#SPMNO2)\n",
    "    - ### 6.4 [RSPM & SPM](#RSPMSPM)\n",
    "    - ### 6.5 [SPM](#SPM)\n",
    "    - ### 6.6 [NO2](#NO2)\n",
    "    - ### 6.7 [RSPM](#RSPM)\n",
    "    - ### 6.8 [SPM & RSPM Interaction](#SPM&RSPM)\n",
    "    - ### 6.9 [NO2 & SPM Interaction](#NO2&SPM)\n",
    "- ## 7. [Regularization](#Regularization)\n",
    "- ## 8. [Conclusion](#Conclusion)\n",
    "- ## 9. [Contribution](#Contribution)\n",
    "- ## 10. [Citation](#Citation)\n",
    "- ## 11. [License](#License)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "15c4cca41bde796f310684c595e3e38e3c37198a"
   },
   "source": [
    "# <p style=\"text-align: center;\">1. Introduction</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "564ee1878db9db0981c831b2a9afb6c866b7e60c"
   },
   "source": [
    "#   1.1 Abstract  <a id='abstract'></a>\n",
    "\n",
    "In this Kernel , The following dataset \"India Air Quality Data\" by Shruti Bhargava has been analyzed. The main focus of this project is learning about modeling of data by supervised algorithms i.e (Linear Regression (regression) and Logistic Regression (classification)). The main focus of this particular kernel is AQI(Air Quality Index), and factors that affects AQI i.e (so2, no2, spm, rspm, pm2_5). Since in the following dataset we have the concentration of pollutants and we need each pollutants index for calculating the air quality index , so that is been calculated further in the process and has been utilised in analysis . Also in the following project there is a brief explanation of how combination of the independent variables (Interaction effect) has what impact on dependent variable and how is the accuracy of the model has been changed because of the same and how interdependence/ correlation (Multicollinearity) between various independent variable has adverse effect on the dependent varaiable and given data model. The solution to the problems of multicollinearity is also been discussed in the following kernel i.e Regularization and Stepwise Regression. Both of which gives us an enhanced model , with better predictors and estimators in alignment with dependent variable. Furthermore, following kernel contains some EDA(Explorartory Data Analysis)E which is usually the first step in your data analysis process. We take a broader look at patterns, trends, outliers, unexpected results and so on in the dataset, using visual and quantitative methods to get a sense of the story it tells. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "178fafc62917563cabc7c43842a126c7f09e34d2"
   },
   "source": [
    "#   1.2 Importing Libraries  <a id='importing_libraries'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "30b6a78a36ef8e0f19c0fc63003b080b62143a84"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import Imputer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 7)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "import statsmodels.formula.api as sm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools import add_constant\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f6b4e1349c17c1f019dc9ad2d5fd735cff46d09e"
   },
   "source": [
    "# 1.3 Dataset Summary <a id='dataset_summary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e7dc95fd1a0f5c6af263a9f9fd0062baa49bc07"
   },
   "source": [
    "### Statistcal analysis of given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f039628a4d7a9625ccefb29f44a439719461702a"
   },
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('../input/data.csv',encoding=\"ISO-8859-1\")\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b8409d74e93a4332be820744a653e388892b947"
   },
   "source": [
    "The dataset consists primarily 5 different types pollutants measured over the years in different states and cities of India. Where *SO2* and *NO2* are harmful gaseous emmissions; *rspm, spm* and *pm2_5* come under susended air pollutants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8e552e3a706b09147f78b4ad566bf069facfc29"
   },
   "source": [
    "### Information about each column and about null values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "661a2c0fc0dae0c9b9cd904e73e37b874fc5e3fd"
   },
   "outputs": [],
   "source": [
    "print(dataset.info())\n",
    "\n",
    "#Now, we can immediatly see that there are quite a few nulls in various columns, \n",
    "#which need work and first need a closer inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fef3fa91b9627fee58484ee2d09a2379e9c70eb6"
   },
   "source": [
    "### The following table shows the first five rows of the given dataset, thereby giving us insight about what sort of dataset it is. And what are the attributes included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "918241daedc814524b1d2709b06d852f38a92c2b"
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e813234945b9419c0e5117bdd284753e5b3a405a"
   },
   "source": [
    "Clearly there are lots of null values, noticeably in stn_code,agency, both of which should therefore be not ncluded further in the analysis.\n",
    "\n",
    "Intuitively, these two columns will hardly add much value to analysis.\n",
    "\n",
    "Now, focusing on the categorical variables, we are left with location_monitoring_station which consists of considerable nulls (approximately 27000).\n",
    "\n",
    "**It would have been useful to have those values for an in depth analysis, but for now we will keep it out because of the null values and come back later if needed.\n",
    "\n",
    "Out of the two dates columns, immediate attention goes to sampling date which has different formats within, highlighting some data input issues.\n",
    "While, it is importnat to have this metric, more useful is to go back to the origin of the dataset and ask relevant questions,as to why are there different formats? Is it a human error or error due to incorporating different formats.For now, we will keep it out and only have the date column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "032636fb6edbd3030c70f0a85c68a2ae266e8364"
   },
   "source": [
    "#   1.4 Dataset Cleaning  <a id='dataset_cleaning'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e7032950e4b1dfad1d6d6d7b9acee2dfac570735"
   },
   "source": [
    "### Following tables gives information about new dataset after dropping of unneccessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9a57289b079264d054b9f7d8b325f0bb4582eb0"
   },
   "outputs": [],
   "source": [
    "dataset.drop(['stn_code','agency','sampling_date','location_monitoring_station'],axis=1,inplace=True)\n",
    "dataset.info()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d96635965b5e154e704592dbaaaccf483935c909"
   },
   "source": [
    "Now that we know that our dataset has missing values, we need to find the columns which has those values alongwith, the percentage effect it has with respect to whole dataset.\n",
    "\n",
    "#### Table Overview:- Following table gives us the column names with the number of missing values and percentage effect it has with respect to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef8c6561d36d6dba7c443c15ac46aa335d6183ff"
   },
   "outputs": [],
   "source": [
    "#Finding missing values in the data set \n",
    "total = dataset.isnull().sum()[dataset.isnull().sum() != 0].sort_values(ascending = False)\n",
    "percent = pd.Series(round(total/len(dataset)*100,2))\n",
    "pd.concat([total, percent], axis=1, keys=['total_missing', 'percent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7be9acc3ac5f152bc906b547c0ba89e89e14ec86"
   },
   "source": [
    "#### Since we don't have any idea about how the data is distributed, and what to take as a measure of central tendency ,it is always advisable to remove outliers. Since outliers has a huge effect on mean , though it does not effect mode and median very much. And usually we use mean as the measure, so we will be removing outliers for the dataset for important columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b1a01af520d180709312d164612641a368ddd7c"
   },
   "outputs": [],
   "source": [
    "def remove_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
    "    #return df_out\n",
    "\n",
    "remove_outlier(dataset,'so2')\n",
    "remove_outlier(dataset,'no2')\n",
    "remove_outlier(dataset,'rspm')\n",
    "remove_outlier(dataset,'spm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf78ca97b02e19a7fa4e102a371cc1f00d7c7328"
   },
   "source": [
    "#### Following table gives statewise distribution of all the major pollutants i.e so2, no2, rspm, spm, pm2_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a8c7b3d1871648983a4393a3237cbb13d7b2c3c0"
   },
   "outputs": [],
   "source": [
    "dataset.groupby('state')[['spm','pm2_5','rspm','so2','no2']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a530119ff947dacd30a77004a430bbe66f289bc2"
   },
   "source": [
    "### Missing values being filled in columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b836be975253e506d608cbfe561676424ef84fdb"
   },
   "source": [
    "Since we already know that our dataset contains missing values , and we need to fill them for our further analysis . We will be using Imputation to fill inour missing values. Imputation is the process of replacing missing data with substituted values . Because missing data can create problems for analyzing data, imputation is seen as a way to avoid pitfalls involved with listwise deletion of cases that have missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb3d8128202a34a7658752cedf937cbe2718fa95"
   },
   "outputs": [],
   "source": [
    "by_State=dataset.groupby('state')\n",
    "\n",
    "def impute_mean(series):\n",
    "    return series.fillna(series.mean())\n",
    "\n",
    "dataset['rspm']=by_State['rspm'].transform(impute_mean)\n",
    "dataset['so2']=by_State['so2'].transform(impute_mean)\n",
    "dataset['no2']=by_State['no2'].transform(impute_mean)\n",
    "dataset['spm']=by_State['spm'].transform(impute_mean)\n",
    "dataset['pm2_5']=by_State['pm2_5'].transform(impute_mean)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#imputer = Imputer(missing_values = 'NaN', strategy = 'median', axis = 0)\n",
    "#imputer = imputer.fit(dataset.groupby('state').iloc[:, 3:8].values)\n",
    "#dataset.iloc[:,3:8] = imputer.transform(dataset.iloc[:, 3:8].values)\n",
    "#dataset.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d3ed8ac42edec5f708dd5e15e5682e0082f1212"
   },
   "source": [
    "#### Understanding the pollutants briefly .\n",
    "\n",
    "NO2: Nitrogen Dioxide and is emmitted mostly from combustion from power sources or transport.\n",
    "\n",
    "SO2: Sulphur Dioxide and is emmitted mostly from coal burning, oil burning, manufacturing of Sulphuric acid.\n",
    "\n",
    "spm: Suspended particulate matter and are known to be the deadliest form of air pollution. They are microscopic in nature and are found to be suspended in earth's atmosphere.\n",
    "\n",
    "rspm: Respirable suspended particulate matter. A sub form of spm and are respnsible for respiratory diseases.\n",
    "\n",
    "pm2_5: Suspended particulate matter with diameters less than 2.5 micrometres. They tend to remain suspended for longer durations and potentially very harmful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf651a3c364bd74db4b3b046a26afa451e66fe68"
   },
   "outputs": [],
   "source": [
    "#Missing values being filled in columns\n",
    "for col in dataset.columns.values:\n",
    "    if dataset[col].isnull().sum() == 0:\n",
    "        continue\n",
    "    if col == 'date':\n",
    "        guess_values = dataset.groupby('state')['date'].apply(lambda x: x.mode().max())\n",
    "    elif col=='type':\n",
    "        guess_values = dataset.groupby('state')['type'].apply(lambda x: x.mode().max())\n",
    "    else:\n",
    "        guess_values = dataset.groupby('state')['location'].apply(lambda x: x.mode().max())\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a3506e6cebfcd7282b0ca91313a93cc3d49883e7"
   },
   "source": [
    "### Derivation for Individual Pollutant Index and AQI\n",
    "\n",
    "The AQI is an index for reporting daily air quality. It tells you how clean or polluted your air is, and what associated health effects might be a concern for you. The AQI focuses on health effects you may experience within a few hours or days after breathing polluted air. EPA\n",
    "calculates the AQI for five major air pollutants regulated by the Clean Air Act: groundlevel ozone, particle pollution Air quality directly affects (also known as particulate our quality of life. matter), carbon monoxide, sulfur dioxide, and nitrogen dioxide. For each of these\n",
    "pollutants, EPA has established national air quality standards to protect public health.\n",
    "\n",
    "AQI is calculated on the range of 0-500, we are scaling the values according to the AQI calculation formula\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b4050b3a313f1d6fdc43ff6f8a7a519639499426"
   },
   "source": [
    "\\begin{equation*}\n",
    "AQI = AQI_{min} +  \\frac{PM_{Obs}-PM_{Min}}{AQI_{Max}-AQI_{Min}}{(PM_{Max}-PM_{Min})}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "326344bc96c053bd2db010f8a0edc618f30dc907"
   },
   "source": [
    "#### Function to calculate so2 individual pollutant index(si)\n",
    "The index category for SO2 is scaled between 0-1600. So on applying formula which is used to calculate AQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5ede8e32917a8a4c5a1762a750af84a4b4324a9"
   },
   "outputs": [],
   "source": [
    "def calculate_si(so2):\n",
    "    si=0\n",
    "    if (so2<=40):\n",
    "     si= so2*(50/40)\n",
    "    elif (so2>40 and so2<=80):\n",
    "     si= 50+(so2-40)*(50/40)\n",
    "    elif (so2>80 and so2<=380):\n",
    "     si= 100+(so2-80)*(100/300)\n",
    "    elif (so2>380 and so2<=800):\n",
    "     si= 200+(so2-380)*(100/420)\n",
    "    elif (so2>800 and so2<=1600):\n",
    "     si= 300+(so2-800)*(100/800)\n",
    "    elif (so2>1600):\n",
    "     si= 400+(so2-1600)*(100/800)\n",
    "    return si\n",
    "dataset['si']=dataset['so2'].apply(calculate_si)\n",
    "df= dataset[['so2','si']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d9ba84990108f7bc4d06f7ef9e232f60aa1c15ab"
   },
   "source": [
    "#### Function to calculate no2 individual pollutant index(ni)\n",
    "The index category for NO2 is scaled between 0-400. So on applying formula which is used to calculate AQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f8e24523fd7ab770ee0fc544960ee59e0a1129a1"
   },
   "outputs": [],
   "source": [
    "#Function to calculate no2 individual pollutant index(ni)\n",
    "def calculate_ni(no2):\n",
    "    ni=0\n",
    "    if(no2<=40):\n",
    "     ni= no2*50/40\n",
    "    elif(no2>40 and no2<=80):\n",
    "     ni= 50+(no2-40)*(50/40)\n",
    "    elif(no2>80 and no2<=180):\n",
    "     ni= 100+(no2-80)*(100/100)\n",
    "    elif(no2>180 and no2<=280):\n",
    "     ni= 200+(no2-180)*(100/100)\n",
    "    elif(no2>280 and no2<=400):\n",
    "     ni= 300+(no2-280)*(100/120)\n",
    "    else:\n",
    "     ni= 400+(no2-400)*(100/120)\n",
    "    return ni\n",
    "dataset['ni']=dataset['no2'].apply(calculate_ni)\n",
    "df= dataset[['no2','ni']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0ccecc0e921817a865f060cdc64fe925bd0c571f"
   },
   "source": [
    "#### Function to calculate rspm individual pollutant index(rpi)\n",
    "The index category for rspm is scaled between 0-400. So on applying formula which is used to calculate AQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "913c7452a05917fd2df3fc255468bfcefd830a7f"
   },
   "outputs": [],
   "source": [
    "#Function to calculate rspm individual pollutant index(rpi)\n",
    "def calculate_(rspm):\n",
    "    rpi=0\n",
    "    if(rpi<=30):\n",
    "     rpi=rpi*50/30\n",
    "    elif(rpi>30 and rpi<=60):\n",
    "     rpi=50+(rpi-30)*50/30\n",
    "    elif(rpi>60 and rpi<=90):\n",
    "     rpi=100+(rpi-60)*100/30\n",
    "    elif(rpi>90 and rpi<=120):\n",
    "     rpi=200+(rpi-90)*100/30\n",
    "    elif(rpi>120 and rpi<=250):\n",
    "     rpi=300+(rpi-120)*(100/130)\n",
    "    else:\n",
    "     rpi=400+(rpi-250)*(100/130)\n",
    "    return rpi\n",
    "dataset['rpi']=dataset['rspm'].apply(calculate_si)\n",
    "df= dataset[['rspm','rpi']]\n",
    "df.head()\n",
    "#df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c04e8c34c5ec23cc1137abeb5ceefc5009ffe220"
   },
   "source": [
    "#### Function to calculate spm individual pollutant index(spi)\n",
    "The index category for rspm is scaled between 0-430. So on applying formula which is used to calculate AQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d930db8e5fbc4be6b6e7312e4db46c259ebfac02"
   },
   "outputs": [],
   "source": [
    "#Function to calculate spm individual pollutant index(spi)\n",
    "def calculate_spi(spm):\n",
    "    spi=0\n",
    "    if(spm<=50):\n",
    "     spi=spm*50/50\n",
    "    elif(spm>50 and spm<=100):\n",
    "     spi=50+(spm-50)*(50/50)\n",
    "    elif(spm>100 and spm<=250):\n",
    "     spi= 100+(spm-100)*(100/150)\n",
    "    elif(spm>250 and spm<=350):\n",
    "     spi=200+(spm-250)*(100/100)\n",
    "    elif(spm>350 and spm<=430):\n",
    "     spi=300+(spm-350)*(100/80)\n",
    "    else:\n",
    "     spi=400+(spm-430)*(100/430)\n",
    "    return spi\n",
    "dataset['spi']=dataset['spm'].apply(calculate_spi)\n",
    "df= dataset[['spm','spi']]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3cae4157509deacb97e0dbda3db45ad219ded793"
   },
   "source": [
    "#### Function to calculate pm2_5 individual pollutant index(pmi)\n",
    "The index category for rspm is scaled between 0-430. So on applying formula which is used to calculate AQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6c35739005cff4302e1aac69c22e8607a57ee450"
   },
   "outputs": [],
   "source": [
    "#Function to calculate pm2_5 individual pollutant index(pmi)\n",
    "def calculate_pmi(pm2_5):\n",
    "    pmi=0\n",
    "    if(pm2_5<=50):\n",
    "     pmi=pm2_5*(50/50)\n",
    "    elif(pm2_5>50 and pm2_5<=100):\n",
    "     pmi=50+(pm2_5-50)*(50/50)\n",
    "    elif(pm2_5>100 and pm2_5<=250):\n",
    "     pmi= 100+(pm2_5-100)*(100/150)\n",
    "    elif(pm2_5>250 and pm2_5<=350):\n",
    "     pmi=200+(pm2_5-250)*(100/100)\n",
    "    elif(pm2_5>350 and pm2_5<=450):\n",
    "     pmi=300+(pm2_5-350)*(100/100)\n",
    "    else:\n",
    "     pmi=400+(pm2_5-430)*(100/80)\n",
    "    return pmi\n",
    "dataset['pmi']=dataset['pm2_5'].apply(calculate_pmi)\n",
    "df= dataset[['pm2_5','pmi']]\n",
    "#df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c74f0bc738f92fab906e614cc58b0a3654a9b37c"
   },
   "source": [
    "#### Function to calculate the air quality index (AQI) of every data value its is calculated as per indian govt standards\n",
    "The purpose of the AQI is to help you understand what\n",
    "local air quality means to your health. Also it is scaled from 0 to 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "19601c8cf4cc73d76ca77f1beb90c7b884f3483e"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url= \"http://airquality.deq.idaho.gov/Information_AQI_files/image002.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a821ee20c7b6514f8da562b1578514d01221b4a3"
   },
   "outputs": [],
   "source": [
    "#function to calculate the air quality index (AQI) of every data value\n",
    "#its is calculated as per indian govt standards\n",
    "def calculate_aqi(si,ni,spi,rpi):\n",
    "    aqi=0\n",
    "    if(si>ni and si>spi and si>rpi):\n",
    "     aqi=si\n",
    "    if(spi>si and spi>ni and spi>rpi):\n",
    "     aqi=spi\n",
    "    if(ni>si and ni>spi and ni>rpi):\n",
    "     aqi=ni\n",
    "    if(rpi>si and rpi>ni and rpi>spi):\n",
    "     aqi=rpi\n",
    "    return aqi\n",
    "dataset['AQI']=dataset.apply(lambda x:calculate_aqi(x['si'],x['ni'],x['spi'],x['rpi']),axis=1)\n",
    "df= dataset[['state','si','ni','rpi','spi','AQI']]\n",
    "df.head()\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b0917a6d58f6ae767f8728eea4a02d6e9dba0ce7"
   },
   "source": [
    "#### *Now that we have a definitive dataset, that is one without null values we can employ various machine learning algorithms to see* *how are dependent and independent variable are related. And also to do Exploratory Data Analysis on the given dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "23589bddbb667d2515b6c56542dcf0e77dd35f7f"
   },
   "source": [
    "#    <p style=\"text-align: center;\">1.5 Exploratory Data Analysis (EDA)  <a id='eda'></a>\n",
    "\n",
    "Exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd300b0d1d6cbb5ec89171a4cc8b8d8e08d4509e"
   },
   "source": [
    "##   1.5.1  Pointplot  <a id='pointplot'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "27a10d5e01a3e4db24500c069a2f49d3e8771f45"
   },
   "source": [
    "### Graph Overview:- Visualization of AQI across india (Year-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d7402d1c40d76d7c865d03aa35e742b0f1523ce"
   },
   "outputs": [],
   "source": [
    "#Visualization of AQI across india \n",
    "\n",
    "dataset['date'] = pd.to_datetime(dataset['date'],format='%Y-%m-%d') # date parse\n",
    "dataset['year'] = dataset['date'].dt.year # year\n",
    "dataset['year'] = dataset['year'].fillna(0.0).astype(int)\n",
    "dataset = dataset[(dataset['year']>0)]\n",
    "\n",
    "df = dataset[['AQI','year','state']].groupby([\"year\"]).median().reset_index().sort_values(by='year',ascending=False)\n",
    "f,ax=plt.subplots(figsize=(15,10))\n",
    "sns.pointplot(x='year', y='AQI', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3f1c1a4d3c51dd6986e4b3b12087df49f7d566a"
   },
   "source": [
    "### Table Overview:- Exploring air pollution state-wise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c7606bbd9a14f6f94e5bb1f1fd18740f727e18a9"
   },
   "outputs": [],
   "source": [
    "#Exploring air pollution state-wise\n",
    "dataset.fillna(0.0,inplace=True)\n",
    "states=dataset.groupby(['state','location'],as_index=False).mean()\n",
    "state=states.groupby(['state'],as_index=False).mean()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0bb19595eb4f307f48243ba4e43dbc509b0d1cec"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(40,40))\n",
    "state_pollution = dataset[['state','so2']].groupby('state').mean()\n",
    "state_pollution.reset_index(inplace = True)\n",
    "state_pollution.sort_values('so2', ascending= False, inplace = True)\n",
    "state_pollution.plot(kind = 'bar', figsize= (20,10), x = 'state', fontsize= 15, title = 'States & Pollutant Levels',ax=axes[0][0])\n",
    "\n",
    "state_pollution_no2 = dataset[['state','no2']].groupby('state').mean()\n",
    "state_pollution_no2.reset_index(inplace = True)\n",
    "state_pollution_no2.sort_values('no2', ascending= False, inplace = True)\n",
    "state_pollution_no2.plot(kind = 'bar', figsize= (20,10), x = 'state', fontsize= 15, title = 'States & Pollutant Levels',ax=axes[0][1])\n",
    "\n",
    "state_pollution_rspm = dataset[['state','rspm']].groupby('state').mean()\n",
    "state_pollution_rspm.reset_index(inplace = True)\n",
    "state_pollution_rspm.sort_values('rspm', ascending= False, inplace = True)\n",
    "state_pollution_rspm.plot(kind = 'bar', figsize= (20,40), x = 'state', fontsize= 15, title = 'States & Pollutant Levels',ax=axes[1][0])\n",
    "\n",
    "\n",
    "state_pollution_spm = dataset[['state','spm']].groupby('state').mean()\n",
    "state_pollution_spm.reset_index(inplace = True)\n",
    "state_pollution_spm.sort_values('spm', ascending= False, inplace = True)\n",
    "state_pollution_spm.plot(kind = 'bar', figsize= (20,40), x = 'state', fontsize= 15, title = 'States & Pollutant Levels',ax=axes[1][1])\n",
    "\n",
    "\n",
    "state_pollution = dataset[['state','pm2_5']].groupby('state').mean()\n",
    "state_pollution.reset_index(inplace = True)\n",
    "state_pollution.sort_values('pm2_5', ascending= False, inplace = True)\n",
    "state_pollution.plot(kind = 'bar', figsize= (20,10), x = 'state', fontsize= 15, title = 'States & Pollutant Levels')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28adb6ec5a7df0b0436e68035034d195068857e7"
   },
   "source": [
    "## 1.5.2 Heatmap  <a id='heatmap'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "51dd3cc895cee4113fa97b18c30240ca91a741ab"
   },
   "source": [
    "Correlation is any statistical association, though in common usage it most often refers to how close two variables are to having a linear relationship with each other. The correlation coefficient r measures the strength and direction of a linear relationship between two variables on a scatterplot. if r>0 higher the correlation and if r<0 correlation is inversely related.\n",
    "\n",
    "### Visual representation in form of heatmap for correlated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "30115568b6be99f8981e66205f378d9a3cf8428f"
   },
   "outputs": [],
   "source": [
    "#correlation\n",
    "dataset.corr()\n",
    "plt.figure(figsize=(16,12))\n",
    "ax=plt.axes()\n",
    "#sns.heatmap(data=fifa_dataset.iloc[:,2:].corr(),annot=True,fmt='.2f',cmap='coolwarm',ax=ax)\n",
    "mask = np.zeros_like(dataset.iloc[:,:].corr())\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    ax = sns.heatmap(data=dataset.iloc[:,:].corr(), mask=mask, vmax=.3, annot=True,fmt='.2f', square=True, cmap='coolwarm')\n",
    "    \n",
    "ax.set_title('Heatmap showing correlated values for the Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d90d6e975befa6bb56a5d0fb567ec2f7df698dc"
   },
   "source": [
    "## 1.5.3 Heatmap Pivot  <a id='heatmp'></a>\n",
    "\n",
    "### AQI By State and Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dd7f62cf610abe33c39f17cc56e4c70cfb61eb6e"
   },
   "outputs": [],
   "source": [
    "#Heatmap Pivot with State as Row, Year as Col, AQI as Value\n",
    "dataset['date'] = pd.to_datetime(dataset['date'],format='%Y-%m-%d') # date parse\n",
    "dataset['year'] = dataset['date'].dt.year # year\n",
    "dataset['year'] = dataset['year'].fillna(0.0).astype(int)\n",
    "dataset = dataset[(dataset['year']>0)]\n",
    "f, ax = plt.subplots(figsize=(40,40))\n",
    "ax.set_title('{} by state and year'.format('AQI'))\n",
    "sns.heatmap(dataset.pivot_table('AQI', index='state',\n",
    "                columns=['year'],aggfunc='mean',margins=True),\n",
    "                annot=True,cmap=\"BuPu\", linewidths=.75, ax=ax,cbar_kws={'label': 'Annual Average'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5941fab01038313aa566a573f9c22715a594baed"
   },
   "source": [
    "## 1.5.4 Regplot  <a id='Regplot'></a>\n",
    "\n",
    "Distribution of important independent variables and their relation with dependent variable i.e AQI is being depicted by the graph. Basically it plots data and a linear regression model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7cc822b74ce3ec36719bf13dde1d68587a0b3dd3"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(34,20))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "z = pd.Series()\n",
    "for col in dataset.columns.values[3:15]:\n",
    "    if ((col!='AQI')&(col!='state')&(col!='location')&(col!='type')&(col!='date')&(col!='year')&(col!='state_label')&(col!='type_label')):\n",
    "      \n",
    "        colums=np.array(dataset[col])\n",
    "        z[col]=colums\n",
    "#p=z.loc[z.index]\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        \n",
    "        #x=z.index.values[i*3+j]\n",
    "        #sns.barplot(z.index[i*3+j],z.values[i*3+j])\n",
    "        #x=z.index.values[i*3+j]\n",
    "        \n",
    "        y_label=z.index[i*5+j]\n",
    "        x_label=z[i*5+j]\n",
    "        \n",
    "        sns.regplot(data=dataset, x=z.index[i*5+j], y='AQI',ax=axes[i,j])\n",
    "\n",
    "\n",
    "fig.suptitle('Distribution of Correlated Factors', fontsize='25')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a01e2495240adc331cf53e26d4488d7b80574064"
   },
   "source": [
    "## 1.5.5 Categorical Conversion  <a id='catconversion'></a>\n",
    "#### Our analysis requires at least one independent variable which needs to be a multi-class categorical variable and a binary categorical variable and its conversion to numeric data.\n",
    "\n",
    "We will be using cat coding and one hot encoding for the same, cat coding converts categorical data into numeric for use , basically it provides numbers for ordinal data. Cat coding creates a mapping of our sortable categories, e. g. old < renovated < new → 0, 1, 2\n",
    "\n",
    "One hot encoding (binary values from categorical data)\n",
    "A one hot encoding is a representation of categorical variables as binary vectors.  This first requires that the categorical values be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.\n",
    "\n",
    "And in our dataset state is a multiclass variable and type is a binary categorical variable. So we are changing them eventually. Further in our analysis we will be using cat coding only to convert our ordinal data.\n",
    "\n",
    "We have mostly used cat coding as it changes the target column itself while in one hot encoding based on number of types the column holds, new columns be created according to the type with binary values i.e 1 and 0 . Thereby making data set much more complex and increasing redundancy. Although both methods be used for categorical to numeric conversion , we have preferred cat coding over one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10570eceeac07dfccb21437e12d8b1b9528e05bf"
   },
   "outputs": [],
   "source": [
    "dataset['state_label'] = dataset['state'].astype('category')\n",
    "cat_columns = dataset.select_dtypes(['category']).columns\n",
    "dataset[cat_columns] = dataset[cat_columns].apply(lambda x: x.cat.codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcd30ce198c89162aa841879fe99b28828d945a4"
   },
   "outputs": [],
   "source": [
    "dataset[\"type_label\"]=dataset[\"type\"].astype(str)\n",
    "dataset[\"type_label\"] = np.where(dataset[\"type\"].str.contains('Residential, Rural and other Areas'), 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7aa23437d50a6dbb66ee1444d133f6cc225db005"
   },
   "source": [
    "#### Table Overview:- Gives the first 5 rows of the dataset with converted categorical variables i.e state_label and type_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0fb1576b837a3fc7847e0288ab4923006c1c2b5"
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d0281c4ed43d81a447457da1fc07de5e176f364f"
   },
   "source": [
    "## <p style=\"text-align: center;\">2. Linear Regression</p> <a id='LinearRegression'></a>\n",
    "\n",
    "Linear regression is basically a linear approach to model the relationship shared between a scalar response (or dependent variable) i.e AQI and one or more explanatory variables (or independent variables) i.e SO2,NO2 etc\n",
    "\n",
    "### Multiple Linear Regression\n",
    "\n",
    "#### The case of multiple explanatory variable (independent variable) is called multiple linear regression.\n",
    "To build a well-performing machine learning (ML) model, it is important to seperate data into training and testing dataset . Basically we are training the model on and testing it against the data that comes from the same set of target distribution. \n",
    "\n",
    "### Simple Linear Regression\n",
    "\n",
    "The case of single explanatory variable (independent variable) is called single linear regression.¶  \n",
    "\n",
    "#### We applied linear regression model on our dataset and calculated the value for Root Mean Squared Error ,Mean Squared Error(log), AIC and BIC\n",
    "\n",
    "Root Mean Squared Error:-\n",
    "Root Mean Square Error (RMSE) mathematically is the standard deviation of the residuals. Residuals is the measure od how far the data points are spreaded across the line of regression which we get by our training data set. RMSE is the measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.\n",
    "\n",
    "##### Mean Squared Error:-\n",
    "The Mean Squared Error (MSE) is a measure of how close a fitted line is to data points. It is the sum, over all the data points, of the square of the difference between the predicted and actual target variables, divided by the number of data points. RMSE is the square root of MSE.\n",
    "\n",
    "##### Akaike information criterion (AIC):-\n",
    "The Akaike information criterion (AIC) is an estimator of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Thus, AIC provides a means for model selection. When a statistical model is used to represent the process that generated the data, the representation will almost never be exact; so some information will be lost by using the model to represent the process. AIC estimates the relative amount of information lost by a given model: the less information a model loses, the higher the quality of that model.\n",
    "\n",
    "In estimating the amount of information lost by a model, AIC deals with the trade-off between the goodness of fit of the model and the simplicity of the model. In other words, AIC deals with both the risk of overfitting and the risk of underfitting.\n",
    ">\n",
    ">\\begin{equation*}\n",
    "AIC = {2}\\cdot\\ {k} + {n}log(RSS/N)       \n",
    "\\end{equation*}\n",
    ">\n",
    "<p style=\"text-align: center;\">where k=number of parameters, RSS=Residual Sum of Squares and n= number of rows</p>\n",
    "\n",
    "##### Bayesian information criterion (BIC) :-\n",
    "It is a criterion for model selection among a finite set of models; the model with the lowest BIC is preferred. It is based, in part, on the likelihood function and it is closely related to the Akaike information criterion (AIC).\n",
    "\n",
    "When fitting models, it is possible to increase the likelihood by adding parameters, but doing so may result in overfitting. Both BIC and AIC attempt to resolve this problem by introducing a penalty term for the number of parameters in the model; the penalty term is larger in BIC than in AIC.\n",
    ">\n",
    "\\begin{equation*}\n",
    "BIC = {n}\\cdot \\ln(RSS/n) + {k}\\cdot\\ln(n)      \n",
    "\\end{equation*}\n",
    ">\n",
    "<p style=\"text-align: center;\">where k=number of parameters, RSS=Residual Sum of Squares and n= number of rows</p>\n",
    "\n",
    " \n",
    "##### P-value:-\n",
    "In statistics, the p-value is a function of the observed sample results (a statistic) that is used for testing a statistical hypothesis. Before the test is performed, a threshold value is chosen, called the significance level of the test, traditionally 5% or 1% and denoted as $\\alpha$.\n",
    "\n",
    "If the p-value is equal to or smaller than the significance level ($\\alpha$), it suggests that the observed data are inconsistent with the assumption that the null hypothesis is true and thus that hypothesis must be rejected (but this does not automatically mean the alternative hypothesis can be accepted as true). When the p-value is calculated correctly, such a test is guaranteed to control the Type I error rate to be no greater than $\\alpha$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e56510230aaf86f5fc6922921d966c837211595"
   },
   "source": [
    "#### Table Overview:- Top 5 rows of the dataset consisting of independent variables that have p-value<0.05\n",
    "And we will be using the following Independent Variables present in the given dataset for modeling our data using linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9d119a6dc12fd43c72c64fde64d52213101d8eca"
   },
   "outputs": [],
   "source": [
    "data_p = dataset.drop(['state', 'location', 'type','date','AQI','year'],axis=1)\n",
    "corr = data_p.corr()\n",
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "selected_column=data_p.columns[columns]                \n",
    "data_p=data_p[selected_column]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee4b20fdac4252f05aaae73925ee1cfa45eb10e5"
   },
   "outputs": [],
   "source": [
    "x_=data_p\n",
    "y=dataset['AQI']\n",
    "#factors with p-value\n",
    "selected_columns_1 = selected_column[0:10].values\n",
    "selected_columns_2=selected_column[11:].values\n",
    "selected_columns=np.concatenate((selected_columns_1,selected_columns_2),axis=0)\n",
    "import statsmodels.formula.api as smf\n",
    "def backwardelimination(x, Y, sl, columns):\n",
    "    numVars = len(x[0])\n",
    "    \n",
    "    for i in range(0, numVars):\n",
    "        regressor_OLS = smf.OLS(Y, x).fit()\n",
    "        maxVar = max(regressor_OLS.pvalues).astype(float)\n",
    "        \n",
    "        if maxVar > sl:\n",
    "          \n",
    "            for j in range(0, numVars - i):\n",
    "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                    x = np.delete(x,j,1)\n",
    "                    columns = np.delete(columns, j)\n",
    "                    \n",
    "    regressor_OLS.summary()\n",
    "    return x, columns\n",
    "SL = 0.05\n",
    "data_modeled, selected_columns = backwardelimination(x_.iloc[:,:].values, y.values, SL,selected_columns )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "360a43c3c9baca9fc056964d3ff8ffabe2db845c"
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "result['AQI'] = dataset.iloc[:,10]\n",
    "#result.head()\n",
    "\n",
    "data_p_selected = pd.DataFrame(data= data_modeled[:,0:9], columns = selected_columns)\n",
    "data_p_selected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "334e0d35ce2c0712d2e5fdc8d1c63d1ce0779204"
   },
   "source": [
    "Since we are to form 3 different linear models for our given project, it would be better if form a function for linear regression that could be utilised everytime we want to perform linear regression . Also, it makes the code less complex and easier to read.\n",
    "\n",
    "We will also be validating our model. \n",
    "Cross Validation:- Cross Validation is a technique which involves reserving a particular sample of a dataset on which we do not train the model. Later, we will test our model on this sample before finalizing it.\n",
    "\n",
    "We will be performing K-Fold Cross Validation.Below are the steps for it:\n",
    "\n",
    "1. Randomly split your entire dataset into k”folds”\n",
    "2. For each k-fold in your dataset, build your model on k – 1 folds of the dataset. Then, test the model to check the effectiveness for kth fold\n",
    "3. Record the error you see on each of the predictions\n",
    "4. Repeat this until each of the k-folds has served as the test set\n",
    "5. The average of your k recorded errors is called the cross-validation error and will serve as your performance metric for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f57dbd9015e7138af6c56e7fd758dbc7ddcb6dca"
   },
   "outputs": [],
   "source": [
    "def linear_regression(X,y):\n",
    "#SPLIT TEST AND TRAIN\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "#One Hot Encoding\n",
    "    X_train = pd.get_dummies(X_train)\n",
    "    X_test = pd.get_dummies(X_test)\n",
    "\n",
    "#Linear Regression\n",
    "    LR = LinearRegression()\n",
    "    LR.fit(X_train, y_train)\n",
    "    predictions = LR.predict(X_test)\n",
    "\n",
    "    print(X_test.shape,X_train.shape,y_test.shape,y_train.shape)\n",
    "    print('r2_Square:%.2f '% r2_score(y_test, predictions))\n",
    "    print('MSE:%.2f '% np.sqrt(mean_squared_error(y_test, predictions)))\n",
    "    print(\"KfoldCrossVal mean score using Linear regression is %s\" %cross_val_score(LR,X_train,y_train,cv=10).mean())\n",
    "    \n",
    "    regressor_OLS = smf.OLS(y_train, X_train).fit()\n",
    "    \n",
    "    plt.figure(figsize=(18,10))\n",
    "    plt.scatter(predictions,y_test,alpha = 0.3)\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('AQI')\n",
    "    plt.title(\"Linear Prediction \")\n",
    "    plt.show()\n",
    "#cross validation    \n",
    "    Kfold = KFold(len(X), shuffle=True)\n",
    "    #X_train = sc.fit_transform(X_train)\n",
    "    #X_test = sc.transform(X_test)\n",
    "    z=print(regressor_OLS.summary())\n",
    "    return z\n",
    "\n",
    "   \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0011657117aa2be4cdfa202ca4238c2ec5085918"
   },
   "outputs": [],
   "source": [
    "####function to calculate cross validation score only\n",
    "def cross_val(X,y):\n",
    "    #SPLIT TEST AND TRAIN\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "#One Hot Encoding\n",
    "    X_train = pd.get_dummies(X_train)\n",
    "    X_test = pd.get_dummies(X_test)\n",
    "\n",
    "#Linear Regression\n",
    "    LR = LinearRegression()\n",
    "    LR.fit(X_train, y_train)\n",
    "    predictions = LR.predict(X_test)\n",
    "    Kfold = KFold(len(X), shuffle=True)\n",
    "    print(\"KfoldCrossVal mean score using Linear regression is %s\" %cross_val_score(LR,X_train,y_train,cv=10).mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "026eaa7a399769282277b6ad541a967f564c9e6a"
   },
   "source": [
    "## 2.1 Linear Model 1 <a id='LinearModel1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1480dc4f830a26bf4f93e412e7743e9ba30bbbd9"
   },
   "source": [
    "The Following plot gives us our first Linear Model along with the summary for the same.\n",
    "\n",
    ">Training Features :- si, ni, rpi, spi, pmi\n",
    "\n",
    ">Target Feature:- AQI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "06d9d9dcb34d30fead36b30a6663eed84faef0b3"
   },
   "outputs": [],
   "source": [
    "X_1=dataset[['si','ni','rpi','spi']]\n",
    "y_1=dataset['AQI']\n",
    "linear_regression(X_1,y_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "37454e32c71431c99b5b1fa2749f6458b59e29ad"
   },
   "source": [
    "## 2.2 Linear Model 2 <a id='LinearModel2'></a>\n",
    "\n",
    "The Following plot gives us our second Linear Model along with the summary for the same.\n",
    "\n",
    ">Training Features :- so2, no2, pm2_5, spm\n",
    "\n",
    ">Target Feature:- AQI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40cf191792401cbf46b6b27478e487fdf63c69cb"
   },
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "X_2= dataset.drop([ 'AQI','state','location','type','rspm','si','ni','rpi','spi','pmi','date','year','state_label','type_label'], axis = 1)\n",
    "y_2 = dataset['AQI']\n",
    "linear_regression(X_2,y_2)\n",
    "#regressor_OLS = smf.OLS(y_train, X_train).fit()\n",
    "#print(\"Model Summary\")\n",
    "#regressor_OLS.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cecf338ec485d6efb31b42ea313a7e810d8d64d3"
   },
   "source": [
    "## 2.3 Linear Model 3 <a id='LinearModel3'></a>\n",
    "\n",
    "The Following plot gives us our second Linear Model along with the summary for the same.\n",
    "\n",
    ">Training Features :- rspm, spm, spi, rpi\n",
    "\n",
    ">Target Feature:- AQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef09a96245535febfe0dff54a57eabbc132acb11"
   },
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "X_3= dataset.drop(['AQI','state','location','type','date','pmi','pm2_5','year','state_label','type_label','so2','no2','si','ni'], axis = 1)\n",
    "y_3 = dataset['AQI']\n",
    "linear_regression(X_3,y_3)\n",
    "#SPLIT TEST AND TRAIN\n",
    "#X_train, X_test, y_train, y_test = train_test_split(dataset_LR_3, target, test_size=0.2)\n",
    "\n",
    "#One Hot Encoding\n",
    "#X_train = pd.get_dummies(X_train)\n",
    "#X_test = pd.get_dummies(X_test)\n",
    "#print(X_test.shape,X_train.shape,y_test.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "902828e259425b54d548b8dcea7f926656858031"
   },
   "source": [
    "#### Significant Questions on the basis of the all 3 Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "071887b18427b4d5005ad1a3fe00eaf3b480c225"
   },
   "source": [
    "##### * Is the relationship significant?   \n",
    "\n",
    "By the results of all three models we can say , that the relationship between the dependent variable (AQI) and significant independent variables(so2,no2,si,ni,pm2_5,rpi,rspm,spm) are significant. \n",
    "\n",
    "##### * Are any model assumptions violated?    \n",
    "\n",
    "Assumptions for Linear Regression are as follow:-\n",
    "1. Linear relationship.\n",
    "2. Multivariate normality.(correlated variables do clustering around mean value)\n",
    "3. No or little multicollinearity.\n",
    "5. No auto-correlation.\n",
    "6. Homoscedasticity.(all random variables in the sequence have the same finite variance)\n",
    "\n",
    "by the results we can say that they have linear relationship and no auto correlation for multicollinearity and multivariate normality please refer to section [Multicollinearity](#Multicollinearity)\n",
    "\n",
    "##### * Does the model make sense? Interpret the meaning of each independent variable.   \n",
    "\n",
    "All three model makes sense.And we can say as the value of each independent variable we have used increase AOI will increase.\n",
    "\n",
    "##### * Cross-validate the model. How well did it do? \n",
    "\n",
    "For all three models cross validation gives good result, such that it is accurate.\n",
    "\n",
    "##### * Compare the AIC, BIC and adjusted R^2.  Do they agree in their ranking of the models?  \n",
    "AIC,BIC and adjusted R^2  for Linear Model 1 is the lowest, lowest and highest , hence it is the the one we should choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "16715f53e081c6df376e3fbf6354ce5051af4950"
   },
   "source": [
    "\n",
    "## <p style=\"text-align: center;\">3. Logistic Regression</p> <a id='LogisticRegression'></a>\n",
    "\n",
    "Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\n",
    "\n",
    "Difference between logistic and linear regression:-\n",
    "In linear regression, the outcome (dependent variable) is continuous. It can have any one of an infinite number of possible values. In logistic regression, the outcome (dependent variable) has only a limited number of possible values(discrete). Also , we use logistic for classification purpose i.e for categorical variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "333f2b987419c17d3eca08a9d362e16785de9fe8"
   },
   "source": [
    "Table Overview:- Following Table gives us dataset with new column i.e AQI_Range that specifies the type of AQI effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb0cc7f6d9bf38e75f16502113e7c5971d71793a"
   },
   "outputs": [],
   "source": [
    "def AQI_Range(x):\n",
    "    if x<=50:\n",
    "        return \"Good\"\n",
    "    elif x>50 and x<=100:\n",
    "        return \"Moderate\"\n",
    "    elif x>100 and x<=200:\n",
    "        return \"Unhealthy for sensitive groups\"\n",
    "    elif x>200 and x<=300:\n",
    "        return \"Unhealthy\"\n",
    "    elif x>300 and x<=400:\n",
    "        return \"Very unhealthy\"\n",
    "    elif x>400:\n",
    "        return \"Hazardous\"\n",
    "\n",
    "dataset['AQI_Range'] = dataset['AQI'] .apply(AQI_Range)\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11a33708f34fa58b920b0607f5e8f9c9511876a9"
   },
   "source": [
    "#### Table Overview:- AQI_Range is converted to AQI_Label according to its 5 types using cat coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c3c7dab60617517b432a243439cfca776e4dd62"
   },
   "outputs": [],
   "source": [
    "dataset['AQI_label'] = dataset['AQI_Range'].astype('category')\n",
    "cat_columns = dataset.select_dtypes(['category']).columns\n",
    "dataset[cat_columns] = dataset[cat_columns].apply(lambda x: x.cat.codes)\n",
    "\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b181f4f439b8c0f883f28a78ab0e6888a766c79a"
   },
   "source": [
    "#### Table Overview:- AQI_Range is converted to AQI_Range_Binary according to its 5 types using custom coding. Since for logistic we require binary categorical values only so we need to use either one hot encoding or custom coding .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7e9f0278fca576f6d16fcbb080d8401faa9fba6b"
   },
   "outputs": [],
   "source": [
    "def AQI_Range_Binary_Label(x):\n",
    "    if x<=200:\n",
    "        return 0\n",
    "    elif x>200:\n",
    "        return 1\n",
    "    \n",
    "dataset['AQI_Range_Binary'] = dataset['AQI'] .apply(AQI_Range_Binary_Label)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6932709ae4b6b09ab44ddd656c10cf2275cabd3c"
   },
   "source": [
    "### Box Plot AQI Dataset Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9613ee332c195b797d2c1dfdab4b8fd9d182fab"
   },
   "outputs": [],
   "source": [
    "dataset_box=dataset.drop(['state', 'location', 'date','state_label',\n",
    "       'type_label','type','AQI_label','AQI_Range_Binary','year','AQI_Range'],axis=1)\n",
    "f, ax = plt.subplots(figsize=(20, 15))\n",
    "\n",
    "ax.set_facecolor('#FFFFFF')\n",
    "plt.title(\"Box Plot AQI Dataset Scaled\")\n",
    "ax.set(xlim=(-10, 250))\n",
    "ax = sns.boxplot(data = dataset_box, \n",
    "  orient = 'h', \n",
    "  palette = 'Set3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e36518dc8585419cafb628c9afb0fa0a246cdbd4"
   },
   "source": [
    "Following gives us the list of predictors that we need to rank in order to see which variables we should use for our logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c4d9786ec30774ef2fb494e8d656b78a4c903132"
   },
   "outputs": [],
   "source": [
    "predictor_names=dataset_box.columns.get_values()\n",
    "predictor_names=predictor_names.tolist()\n",
    "predictor_names.pop()\n",
    "predictor_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "06471d37d3f168ad6028195b3ffda6fc8fc5448c"
   },
   "source": [
    "Following gives us the dictionary of predictors that have been ranked in order to see which variables we should use for our logistic model. We are using median as to rank the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e80af9fd54887d0491878644129b9e546a0bbe14"
   },
   "outputs": [],
   "source": [
    "def rank_predictors(dat,l,f='AQI_Range_Binary'):\n",
    "    rank={}\n",
    "    max_vals=dat.max()\n",
    "    median_vals=dat.groupby(f).median()  # We are using the median \n",
    "    for p in l:\n",
    "        score=np.abs((median_vals[p][1]-median_vals[p][0])/max_vals[p])\n",
    "        rank[p]=score\n",
    "    return rank\n",
    "cat_rank=rank_predictors(dataset,predictor_names) \n",
    "cat_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ffb2a545fe033abf90309287320046c54207fca"
   },
   "source": [
    "Sorted List of Ranked Predictors depending on previous function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c14aa7f877ade7594ebc0e0c238660eeeafa9960"
   },
   "outputs": [],
   "source": [
    "# Take the top predictors based on mean difference\n",
    "cat_rank=sorted(cat_rank.items(), key=lambda x: x[1],reverse= True)\n",
    "\n",
    "ranked_predictors=[]\n",
    "for f in cat_rank:\n",
    "    ranked_predictors.append(f[0])\n",
    "ranked_predictors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "711dd56333e2a1e499bcfb5cc5f4676c40739a37",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_log=dataset.drop(['state','type','date','location'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de088adf9dbe941d580d81fdffe11647d9530eb3"
   },
   "source": [
    "Since we are to form 3 different logistic models for our given project, it would be better if form a function for logistic regression that could be utilised everytime we want to perform logistic regression . Also, it makes the code less complex and easier to read.\n",
    "\n",
    "We will also be validating our model. Cross Validation:- Cross Validation is a technique which involves reserving a particular sample of a dataset on which we do not train the model. Later, we will test our model on this sample before finalizing it.\n",
    "\n",
    "We will be performing K-Fold Cross Validation.Below are the steps for it:\n",
    "\n",
    "Randomly split your entire dataset into k”folds”\n",
    "For each k-fold in your dataset, build your model on k – 1 folds of the dataset. Then, test the model to check the effectiveness for kth fold\n",
    "Record the error you see on each of the predictions\n",
    "Repeat this until each of the k-folds has served as the test set\n",
    "The average of your k recorded errors is called the cross-validation error and will serve as your performance metric for the model.\n",
    "\n",
    "    \n",
    "    \n",
    "##### Predicting the Test set results(Classification Report - Interpreting meaning of values we get in it)\n",
    "Precision – Accuracy of positive predictions.\n",
    "\n",
    "\\begin{equation*}\n",
    "Precision =(\\frac{TP}{TP+FP})   \n",
    "\\end{equation*}\n",
    "\n",
    "FN – False Negatives\n",
    "Recall (aka sensitivity or true positive rate): Fraction of positives That were correctly identified.\n",
    "\n",
    "\\begin{equation*}\n",
    "Recall =(\\frac{TP}{TP+FN})   \n",
    "\\end{equation*}\n",
    "\n",
    "F1 Score (aka F-Score or F-Measure) – A helpful metric for comparing two classifiers. F1 Score takes into account precision and the recall. It is created by finding the the harmonic mean of precision and recall.\n",
    "\n",
    "\\begin{equation*}\n",
    "F1 = {2}\\times(\\frac{precision\\times recall}{precision + recall})   \n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "AUC/ROC curve:- The ROC curve is a fundamental tool for diagnostic test evaluation.\n",
    "In a ROC curve the true positive rate (Sensitivity) is plotted in function of the false positive rate (100-Specificity) for different cut-off points of a parameter. Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold. The area under the ROC curve (AUC) is a measure of how well a parameter can distinguish between two diagnostic groups (diseased/normal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "43a3933b5f67ba313ac56bc7c3828a5fc13d5e0c"
   },
   "outputs": [],
   "source": [
    "def logistic_regression(x,y):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=0)\n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    # Feature scaling\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    x_test = sc.fit_transform(x_test)\n",
    "    \n",
    "    #Fitting logistic regression to the training set\n",
    "    classifier = LogisticRegression(random_state = 0)\n",
    "    classifier.fit(x_train,y_train)\n",
    "    \n",
    "    \n",
    "    # Logistic regression cross validation\n",
    "    #Kfold = KFold(len(ranked_predictors), shuffle=False)\n",
    "    #print(\"KfoldCrossVal mean score using Logistic regression is %s \\n\" %cross_val_score(classifier,x,y,cv=10).mean())\n",
    "    k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    cvs=cross_val_score(classifier, x_train, y_train, cv=k_fold).mean()\n",
    "    print(\"KfoldCrossVal mean score using Logistic regression is %s \\n\"%cvs)\n",
    "    \n",
    "    print(\"Logistic Analysis Report\")\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(y_pred)\n",
    "    #Accuracy score\n",
    "    print (\"Accuracy Score:%.2f\" % metrics.accuracy_score(y_test,classifier.predict(x_test)))\n",
    "    #probabilty of dependent variable\n",
    "    y_pred_proba = classifier.predict_proba(x_test)[::,1]\n",
    "    print('Probabilty of dependent variable')\n",
    "    print(y_pred_proba.mean())\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.legend(loc=4)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.show()\n",
    "   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78354064e8926c5c0ae6d6155d2e2a46b05e59dc"
   },
   "outputs": [],
   "source": [
    "def logit_summary(y,X):\n",
    "    logit_model=sm.Logit(y,X)\n",
    "    result=logit_model.fit()\n",
    "    print(\"Model Summary\")\n",
    "    print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "120d5606dce51bdf2d2dc2adab625f289520297e"
   },
   "source": [
    "## 3.1 Logistic Model 1 <a id='LogisticModel1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "825212b823e9e5b1b02c7950d07d1dd563883d03"
   },
   "source": [
    "The Following plot gives us our first Logistic Model along with the summary for the same.\n",
    "\n",
    ">Training Features :- 'so2', 'no2', 'rspm', 'spm', 'pm2_5', 'si', 'ni', 'rpi', 'spi', 'pmi',\n",
    "       'year', 'state_label', 'type_label'\n",
    "\n",
    ">Target Feature:- AQI_Range_Binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "578629c2276b27353cb48414f262c5326916cb0b"
   },
   "outputs": [],
   "source": [
    "data_log_1=dataset.drop(['state','type','date','location','AQI_label','AQI_Range','AQI','AQI_Range_Binary'],axis=1)\n",
    "x=data_log_1.iloc[:,:]\n",
    "y=data_log.iloc[:,16]\n",
    "\n",
    "logistic_regression(x,y)\n",
    "logit_summary(y,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "474355ea8e0bf98cebd651eac974be7d1b3b2f4c"
   },
   "source": [
    "## 3.2 Logistic Model 2 <a id='LogisticModel2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ca257acfd3b61f3b41a1258e979befa98a347d7"
   },
   "source": [
    "The Following plot gives us our first Logistic Model along with the summary for the same.\n",
    "\n",
    ">Training Features :- 'so2', 'no2', 'rspm', 'spm', 'pm2_5', 'si', 'ni', 'rpi', 'spi', 'pmi',\n",
    "       'year', 'state_label', 'AQI_Range_Binary'\n",
    "\n",
    ">Target Feature:- 'type_label'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e5df78f5ff8ab8119ca0fb96d9e7e5076dc2128"
   },
   "outputs": [],
   "source": [
    "#Logistic Regression Model 2\n",
    "data_log_2=dataset.drop(['state','type','date','location','AQI_Range','type_label','AQI_label','AQI'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "30fa86fd4043278d125ae1c7017346dd97600ed5"
   },
   "outputs": [],
   "source": [
    "x=data_log_2.iloc[:,:]\n",
    "y=dataset['type_label']\n",
    "logistic_regression(x,y)\n",
    "logit_summary(y,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fac948d8f5b2e93cc272d820b0ca780caedc501f"
   },
   "source": [
    "## 3.3 Logistic Model 3 <a id='LogisticModel3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14a5cb88dfcb80e9394d64d040dd1da8cfbb39e2"
   },
   "source": [
    "The Following plot gives us our first Logistic Model along with the summary for the same.\n",
    "\n",
    ">Training Features :- predictor_names\n",
    "\n",
    ">Target Feature:- Type_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9c3eab2a4750203d43ad6df056cd847b79bfbc7"
   },
   "outputs": [],
   "source": [
    "x=dataset[predictor_names]\n",
    "y=data_log.iloc[:,13]\n",
    "logistic_regression(x,y)\n",
    "logit_summary(y,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1600a6d9b8b9a8ab3233c7f6c5bc3f71bf6ae26d"
   },
   "source": [
    "#### Significant Questions on the basis of the all 3 Logistic Models\n",
    "##### * Is the relationship significant?   \n",
    "\n",
    "Relationship between AQI_Range_Binary and all other pollutants and their indexes(independent variable) are significant as we can see the first Logistic Model 1 .\n",
    "Relationship between type_label(Dependent Variable) and other independent variable is somewhat relatable though we cannot say for sure they are highly related or not.\n",
    "\n",
    "##### * Are any model assumptions violated?      \n",
    "\n",
    "The logistic regression method assumes that:\n",
    "\n",
    "1. The outcome is a binary or dichotomous variable like yes vs no, positive vs negative, 1 vs 0.\n",
    "2. There is a linear relationship between the logit of the outcome and each predictor variables. Recall that the logit function is logit(p) = log(p/(1-p)), where p is the probabilities of the outcome (see Chapter @ref(logistic-regression)).\n",
    "3. There is no influential values (extreme values or outliers) in the continuous predictors\n",
    "4. There is no high intercorrelations (i.e. multicollinearity) among the predictors.\n",
    "\n",
    "by the results we can say that outcome is binary. Since we have scaled the dataset , there are no outliers and as for multicollinearity please refer to section [Multicollinearity](#Multicollinearity)\n",
    "\n",
    "\n",
    "##### * Does the model make sense?  Interpret the meaning of each independent variable.  \n",
    "\n",
    "Except for model 1 in which dependent variable is AQI_Range_Binary accuracy is 99% , so we can say that model makes sense . Since with the value of each independent variable the dependent is increasing.\n",
    "But for model 2 and model 3 , the dependent variable is Type_label and accuracy is less , so though it does make sense but a better model can be used in its place as well.\n",
    "\n",
    "##### * Cross-validate the model. How well did it do? \n",
    "Cross Validation of each model is done with each models generation. And according to cross validation score only first model gives an accurate score .\n",
    "\n",
    "##### * Calculate the probability of getting the dependent variable\n",
    "Done within modeling. Please see each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4e5abb8ad824e2d60327b1ca0833fc99e8943330"
   },
   "source": [
    "## <p style=\"text-align: center;\">4. Multicollinearity</p> <a id='Multicollinearity'></a>\n",
    "\n",
    "In statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.\n",
    "\n",
    "Why Multicollinearity is a problem?\n",
    ">Multicollinearity occurs when independent variables in a regression model are correlated. This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results.\n",
    "\n",
    "VIF assesses whether factors are correlated to each other (multicollinearity)\n",
    "High variance inflation factor means that they \"have\" the same variance within this dataset. We would need to discard one of these variables before moving on to model building or risk building a model with high multicolinearity.\n",
    "If the VIF is equal to 1 there is no multicollinearity among factors, but if the VIF is greater than 1, the predictors may be moderately correlated. If the VIF for a factor is near or above 5 we may have to remove highly correlated factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a82d7be24b4f8800c54f00d89141bfa69acf30f5"
   },
   "outputs": [],
   "source": [
    "def variance_IF(X):\n",
    "    vif=vif = pd.DataFrame()\n",
    "    vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    vif[\"features\"] = X.columns\n",
    "    return vif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8db24f37634cef6286c1e5000977355e4728f58"
   },
   "source": [
    "#### Table Overview:- For Linear Model 1 we can see that VIF for si, ni is fine but for rpi and spi it is greater than 5. So we can remove one of them and can see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4133573ab916b6aee8ee14a62285ed7b30d0a845"
   },
   "outputs": [],
   "source": [
    "##For first Linear Model\n",
    "variance_IF(X_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1135ab33907f30bfa55a2f5d61136cea5eafcfa8"
   },
   "source": [
    "#### Table Overview:- For Linear Model 2 we can see that VIF for so2, no2 ,spm and pm2_5 is fine. So model needs no change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "735a32a141d89251daea0b25803c5c723c609080"
   },
   "outputs": [],
   "source": [
    "##For second Linear Model\n",
    "variance_IF(X_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea592f694200416f9a9ea153984633b3f1ba1a7b"
   },
   "source": [
    "#### Table Overview:- For Linear Model 3 we can see that VIF for rspm, rpi ,spm and spi is much greater than 5. So model needs to be changed on a whole basis.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98327b04e2d4e374212d7c55fc79da78e93f06a0"
   },
   "outputs": [],
   "source": [
    "##For third Linear Model\n",
    "variance_IF(X_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7268ef31b4c26d09e55f94a2ae270eecb949fabc"
   },
   "source": [
    "#### Significant Questions on the basis of the all 3 Linear Models\n",
    "##### * Is there any multi-colinearity in the model?   \n",
    "\n",
    "Yes there is multicollinearity in all models except for linear model 2 .(based on VIF values calculated)\n",
    "\n",
    "##### * In the multiple regression models are predictor variables independent of all the other predictor variables?  \n",
    "For Model 1:- spi and rpi are nor exactly independent of all other predictor variables , since the calculated VIF value is greater than 5.\n",
    "\n",
    "For Model 2:- Model seems fine, because all the independent variables are not that related since VIF<5.\n",
    "\n",
    "For Model 3:- The worst model out of all , all the independent variables are dependent on each other highly because VIF value for all are much higher than expected.\n",
    "\n",
    "##### * In multiple regression models rank the most significant predictor variables and exclude insignificant ones from the model. \n",
    "\n",
    "The regular regression coefficients that we see in statistical output describe the relationship between the independent variables and the dependent variable. The coefficient value represents the mean change of the dependent variable given a one-unit shift in an independent variable Consequently, we might feel that we can use the absolute sizes of the coefficients to identify the most important variable. After all, a larger coefficient signifies a greater change in the mean of the independent variable. However, the independent variables can have dramatically different types of units, which make comparing the coefficients meaningless.\n",
    "\n",
    "Calculations for p-values include various properties of the variable, but importance is not one of them. A very small p-value does not indicate that the variable is important in a practical sense. \n",
    "\n",
    "We have already excluded the variables in the previous answer and about ranking the significant variables. Output for stepwise regression gives us the important variables. [Stepwise Regression](#StepwiseRegression)\n",
    "\n",
    "##### * Cross-validate the models. How well did they do?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "23d6878dfbf07d8584ca92b7d2928b4f74f75636"
   },
   "outputs": [],
   "source": [
    "#Linear Model 1:-\n",
    "X_M1=X_1[['si','ni']]\n",
    "print('Linear Model 1')\n",
    "cross_val(X_M1,y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9733cca6985b4e9e49a142bec663c84ee8eda2d1"
   },
   "outputs": [],
   "source": [
    "#Linear Model 2:-\n",
    "print('Linear Model 2')\n",
    "cross_val(X_2,y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "985217a2ac709919fa6f8124e56ec6396f0bdae3"
   },
   "outputs": [],
   "source": [
    "#Linear Model 2:-\n",
    "print('Linear Model 3')\n",
    "cross_val(X_3,y_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6bf8c5785945f2a7ba95c3d2cbee4c53ce5bbd1"
   },
   "source": [
    "## <p style=\"text-align: center;\">5. Stepwise Regression</p> <a id='StepwiseRegression'></a>\n",
    "\n",
    "Stepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. (p-value in our case). \n",
    "\n",
    ">BACKWARD STEPWISE REGRESSION is a stepwise regression approach, that begins with a full model and at each step gradually eliminates variables from the regression model to find a reduced model that best explains the data. Also known as Backward Elimination regression.\n",
    "\n",
    ">FORWARD STEPWISE REGRESSION is a type of stepwise regression which begins with an empty model and adds in variables one by one. ... It is one of two commonly used methods of stepwise regression; the other is backward elimination, and is almost opposite.Se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dedd8592d227bba6812028dc60e0c714813fe962"
   },
   "outputs": [],
   "source": [
    "#https://datascience.stackexchange.com/questions/937/does-scikit-learn-have-forward-selection-stepwise-regression-algorithm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X=X_2.astype(float)\n",
    "y=dataset['AQI']\n",
    "\n",
    "\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        \n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.argmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included\n",
    "\n",
    "result = stepwise_selection(X, y)\n",
    "\n",
    "print('resulting features:')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8b21bbcd5e78f621a8ae904b9b614708fff702a9"
   },
   "source": [
    "## Cross Validation of Linear Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0a3ca4b28a9669fe001b550e896aebbe0ce36107"
   },
   "outputs": [],
   "source": [
    "dataset_LR_2=dataset[['so2', 'spm', 'no2', 'pm2_5']]\n",
    "target=dataset['AQI']\n",
    "linear_regression(dataset_LR_2,target)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(dataset_LR_2, target, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5e1b0a45b561886ee7f7608e9a6b52a6df3eaa21"
   },
   "source": [
    "## <p style=\"text-align: center;\">6. Interaction Effect</p> <a id='InteractionEffect'></a>\n",
    "Interaction effects occur when the effect of one variable depends on the value of another variable. Interaction effects are common in regression analysis. In any study, many variables can affect the outcome. Changing these variables can affect the outcome directly. In more complex study areas, the independent variables might interact with each other. Interaction effects indicate that a third variable influences the relationship between an independent and dependent variable. This type of effect makes the model more complex, but if the real world behaves this way, it is critical to incorporate it in your model. \n",
    "\n",
    "Here we are taking 3 independant variables and seeing their individual standard error, t score and P values, and these values in presence of each other. And finally we see interaction of age and potential. The 3 independent variables are Age, International Reputation and Potential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5dd0502e36d4fe731fbd5235284cc4c69ff7a6de"
   },
   "source": [
    "Function to calculate the RSS and R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2fbadcfa97f056b58413b33f90c9121ec06ee623"
   },
   "outputs": [],
   "source": [
    "def evaluateModel (model):\n",
    "    print(\"RSS = \", ((dataset.AQI - model.predict())**2).sum())\n",
    "    print(\"R2 = \", model.rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dfe0862d087e8405e86a775105e061234122d5c"
   },
   "source": [
    "### 6.1 Interaction Effect-All Columns<a id='AllColumns'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ad45cdb851585d02f7a15e2399b96924b546328"
   },
   "outputs": [],
   "source": [
    "modelAll = smf.ols('AQI ~ no2 + rspm + spm', dataset).fit()\n",
    "print(modelAll.summary().tables[1])\n",
    "evaluateModel(modelAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "02cc9a7e7ba7230b2cd2fdd5d15eff8cde946cb2"
   },
   "source": [
    "### 6.2 Interaction Effect-NO2 & RSPM<a id='NO2RSPM'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f6397f34cae46b88d33724c583349b34632aed69"
   },
   "outputs": [],
   "source": [
    "modelNo2_rspm = smf.ols('AQI ~ no2 + rspm ', dataset).fit()\n",
    "print(modelNo2_rspm.summary().tables[1])\n",
    "evaluateModel(modelNo2_rspm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8a44f75a469dbd72d6d7b79fb10f7a35cfef652d"
   },
   "source": [
    "### 6.3 Interaction Effect-NO2 & SPM<a id='NO2SPM'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21f1fa2c6d6de486ee4f2c2e3dc6930ab6b7073e"
   },
   "outputs": [],
   "source": [
    "modelNo2_spm = smf.ols('AQI ~ no2 + spm ', dataset).fit()\n",
    "print(modelNo2_spm.summary().tables[1])\n",
    "evaluateModel(modelNo2_spm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f6ee1d02d4b2a39ec653de7f43fa1ba9abf61902"
   },
   "source": [
    "### 6.4 Interaction Effect-SPM & RSPM<a id='RSPMSPM'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e7e11177b60d9af4f57060f30bc5fb023cbdc22"
   },
   "outputs": [],
   "source": [
    "modelspm_rspm = smf.ols('AQI ~ spm + rspm ', dataset).fit()\n",
    "print(modelspm_rspm.summary().tables[1])\n",
    "evaluateModel(modelspm_rspm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8cdb7223a2a39e6b28cd1d9d52da7b59a34d40e9"
   },
   "source": [
    "### 6.5 Interaction Effect-SPM<a id='SPM'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e93b1a51baf86dd6261caa0f4ab27fa58244935d"
   },
   "outputs": [],
   "source": [
    "model_spm = smf.ols('AQI ~ spm', dataset).fit()\n",
    "print(model_spm.summary().tables[1])\n",
    "evaluateModel(model_spm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07241bc0c4e8e3f83e4e3be3ef8eef38b1136464"
   },
   "source": [
    "### 6.6 Interaction Effect-NO2<a id='NO2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd52f3b36b9749aff55cb65fd58b6bffe3426d21"
   },
   "outputs": [],
   "source": [
    "model_no2 = smf.ols('AQI ~ no2', dataset).fit()\n",
    "print(model_no2.summary().tables[1])\n",
    "evaluateModel(model_no2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6aa2ae9428202a2dbc8c07a065341440fd481201"
   },
   "source": [
    "### 6.7 Interaction Effect-RSPM<a id='RSPM'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da8e2f907e415d845ff4c025cb6feb0c7dfc6909"
   },
   "outputs": [],
   "source": [
    "model_rspm = smf.ols('AQI ~ rspm', dataset).fit()\n",
    "print(model_rspm.summary().tables[1])\n",
    "evaluateModel(model_rspm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c76c9d4f486b1a04c7dfae8d277fc720fc081317"
   },
   "source": [
    "### 6.8 Interaction Effect-SPM*RSPM<a id='SPM&RSPM'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "889ed494bb4063824b80b610401e3d3897ff3eee"
   },
   "outputs": [],
   "source": [
    "modelSynergy = smf.ols('AQI ~ spm + rspm + spm*rspm', dataset).fit()\n",
    "print(modelSynergy.summary().tables[1])\n",
    "evaluateModel(modelSynergy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "85a3c533aa6f62c9a9474416dd7c5880418a7194"
   },
   "source": [
    "### 6.9 Interaction Effect-NO2* SPM<a id='NO2&SPM'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b772bf20d7d1c39954f58ad0c6e563aebd35ebde"
   },
   "outputs": [],
   "source": [
    "modelSynergy = smf.ols('AQI ~ no2 + spm + no2*spm', dataset).fit()\n",
    "print(modelSynergy.summary().tables[1])\n",
    "evaluateModel(modelSynergy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6548a4c1d94fe3200182d6671edd6e1005f94c90"
   },
   "source": [
    "## <p style=\"text-align: center;\">7. Regularization</p> <a id='Regularization'></a>\n",
    "\n",
    "Regularization is a very important technique in machine learning to prevent overfitting. Mathematically speaking, it adds a regularization term in order to prevent the coefficients to fit so perfectly to overfit. The difference between the L1 and L2 is just that L2 is the sum of the square of the weights, while L1 is just the sum of the weights\n",
    "\n",
    "Is there collinearity among some features? L2 regularization can improve prediction quality in this case, as implied by its alternative name, \"ridge regression.\" However, it is true in general that either form of regularization will improve out-of-sample prediction, whether or not there is multicollinearity and whether or not there are irrelevant features, simply because of the shrinkage properties of the regularized estimators. L1 regularization can't help with multicollinearity it will just pick the feature with the largest correlation to the outcome  (which isn't useful if you have an interest in estimating coefficients for all features which are strongly correlated with your target). Ridge regression can obtain coefficient estimates even when you have more features than examples... but the probability that any will be estimated precisely at 0 is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d79cac6a2a2ebd5cc2445674b20360025151ea44"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Regularization:- l2\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "#for i in range(0, 1): Matrix\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_1,y_1, test_size=0.2)\n",
    "X_train = pd.get_dummies(X_train)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "\n",
    "ridgeReg = Ridge(alpha=0.05, normalize=True)\n",
    "ridgeReg.fit(X_train,y_train)\n",
    "pred = ridgeReg.predict(X_test)\n",
    "\n",
    "print(X_test.shape,X_train.shape,y_test.shape,y_train.shape)\n",
    "print('r2_Square:%.2f '% r2_score(y_test, pred))\n",
    "print('MSE:%.2f '% np.sqrt(mean_squared_error(y_test, pred)))\n",
    "\n",
    "regressor_OLS = smf.OLS(y_train, X_train).fit()\n",
    "\n",
    "plt.figure(figsize=(18,10))\n",
    "plt.scatter(pred,y_test,alpha = 0.3)\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('AQI')\n",
    "plt.title(\"Linear Prediction \")\n",
    "plt.show()\n",
    "#cross validation    \n",
    "Kfold = KFold(len(X_1), shuffle=True)\n",
    "    #X_train = sc.fit_transform(X_train)\n",
    "    #X_test = sc.transform(X_test)\n",
    "print(\"KfoldCrossVal mean score using Linear regression is %s\" %cross_val_score(ridgeReg,X_train,y_train,cv=10).mean())\n",
    "\n",
    "\n",
    "print(regressor_OLS.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e0b62699ecb439a22d337703536e95c2f60b7e1a"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create an array of alphas and lists to store scores\n",
    "alpha_space = np.logspace(-4, 0, 50)\n",
    "ridge_scores = []\n",
    "ridge_scores_std = []\n",
    "\n",
    "# Create a ridge regressor: ridge\n",
    "ridge = Ridge(normalize=True)\n",
    "\n",
    "# Compute scores over range of alphas\n",
    "for alpha in alpha_space:\n",
    "\n",
    "    # Specify the alpha value to use: ridge.alpha\n",
    "    ridge.alpha = alpha\n",
    "    \n",
    "    # Perform 10-fold CV: ridge_cv_scores\n",
    "    ridge_cv_scores = cross_val_score(ridge, X_train, y_train, cv=10)\n",
    "    \n",
    "    # Append the mean of ridge_cv_scores to ridge_scores\n",
    "    ridge_scores.append(np.mean(ridge_cv_scores))\n",
    "    \n",
    "    # Append the std of ridge_cv_scores to ridge_scores_std\n",
    "    ridge_scores_std.append(np.std(ridge_cv_scores))\n",
    "\n",
    "# Use this function to create a plot    \n",
    "def display_plot(cv_scores, cv_scores_std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(alpha_space, cv_scores)\n",
    "    \n",
    "    std_error = cv_scores_std / np.sqrt(10)\n",
    "\n",
    "    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n",
    "    ax.set_ylabel('CV Score +/- Std Error')\n",
    "    ax.set_xlabel('Alpha')\n",
    "    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n",
    "    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n",
    "    ax.set_xscale('log')\n",
    "    plt.show()\n",
    "\n",
    "# Display the plot\n",
    "display_plot(ridge_scores, ridge_scores_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ddd082287accb0eb98ba7395e0107f0535eea6ed"
   },
   "outputs": [],
   "source": [
    "\"\"\"X_train, X_test, y_train, y_test = train_test_split(X_3,dataset['AQI'], test_size=0.2)\n",
    "X_train = pd.get_dummies(X_train)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "\n",
    "lassoReg = Lasso(alpha=0.3, normalize=True)\n",
    "lassoReg.fit(X_train,y_train)\n",
    "\n",
    "print(X_test.shape,X_train.shape,y_test.shape,y_train.shape)\n",
    "print('r2_Square:%.2f '% r2_score(y_test, pred))\n",
    "print('MSE:%.2f '% np.sqrt(mean_squared_error(y_test, pred)))\n",
    "\n",
    "regressor_OLS = smf.OLS(y_train, X_train).fit()\n",
    "\n",
    "plt.figure(figsize=(18,10))\n",
    "plt.scatter(pred,y_test,alpha = 0.3)\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('AQI')\n",
    "plt.title(\"Linear Prediction \")\n",
    "plt.show()\n",
    "#cross validation    \n",
    "Kfold = KFold(len(X), shuffle=True)\n",
    "    #X_train = sc.fit_transform(X_train)\n",
    "    #X_test = sc.transform(X_test)\n",
    "print(\"KfoldCrossVal mean score using Linear regression is %s\" %cross_val_score(lassoReg,X_train,y_train,cv=10).mean())\n",
    "\n",
    "\n",
    "regressor_OLS.summary()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "737964d20b8d56b7a1c54182f743c09985a50c43"
   },
   "source": [
    "## <p style=\"text-align: center;\">8. Conclusion</p> <a id='Conclusion'></a>\n",
    "1. We can say that on the basis of AIC, BIC and R^2 value for linear regression Model 1 is the best , since it has low values overall. \n",
    "2. Lower the value of AIC and BIC means better model since both measure the loss of data while modeling data, and low value denotes less data is lost overall.\n",
    "3. AQI is highly correlated with all the independent variables(so2, no2, rspm, spm and pm2_5) .\n",
    "4. AQI has been increasing over the years.\n",
    "5. As for logistic regression , only model 1 provides us with accurate results more so because AQI_Range_Binary is the dependent variable we used . But for model 2 and model 3 dependent variable is type_label(sort of area) and the accuracy results are comparitively lower so we can say that though the factors are related to type label in a way but there relations are not enough to be used as for prediction and estimation purposes.\n",
    "6. As for significant variables concerned we cannot use regular regression coefficients and p-value to calculate the same(explained above).\n",
    "7. Our dataset also contains multicollinearity, all the independent variables are somewhat related to each other as we can see in our results.\n",
    "8. After concluding that Multicollinearity do exist in our dataset, and when we try to remove highly multicollinear variables the value of R^2 drops , thus making us conclude that our dataset is not fit for both linear regression and logistic regression (because assumptions for same are violated).\n",
    "9. After stepwise regression, we conclude that the most significant variables that should be used for regression with our dependent variable are 'so2', 'spm', 'no2', 'pm2_5' (with p-value < 0.05).\n",
    "10. Regularization as such as no effect on the model, though there is a slight increase in the accuracy and cross_val_score but it is not that big that we should do it.\n",
    "11. Interaction effect for (spm*rspm) is maximum and that we have utilised and and has gotten good R^2 value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "347995bde0bf73612b8e3bf8f5e7b53f59d5d3d6"
   },
   "source": [
    "## <p style=\"text-align: center;\">9. Contribution</p> <a id='Contribution'></a>\n",
    "\n",
    "I contributed around 65% in terms of coding for the given assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0a6cce84cb35b10cca8c24e9b110e5b0b0c80d97"
   },
   "source": [
    "## <p style=\"text-align: center;\">10. Citation</p> <a id='Citation'></a>\n",
    "\n",
    "1. http://joshlawman.com/metrics-classification-report-breakdown-precision-recall-f1/\n",
    "2. https://github.com/nikbearbrown/INFO_6105/tree/master/Week_2\n",
    "3. https://www.kaggle.com/anbarivan/indian-air-quality-analysis-prediction-using-ml\n",
    "4. https://datascience.stackexchange.com/questions/937/does-scikit-learn-have-forward-selection-stepwise-regression-algorithm\n",
    "5. https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/\n",
    "6. https://www.kaggle.com/marcogdepinto/feature-engineering-eda-data-cleaning-tutorial\n",
    "7. https://www.analyticsvidhya.com/blog/2018/05/improve-model-performance-cross-validation-in-python-r/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e6939d80f09d6d3447ca6e22e821313ceff178a5"
   },
   "source": [
    "## <p style=\"text-align: center;\">11. License</p> <a id='License'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db0ecbe8eb475d4f2508edc8668ca4b52d6ca592"
   },
   "source": [
    "Copyright (c) 2019 Manali Sharma\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
